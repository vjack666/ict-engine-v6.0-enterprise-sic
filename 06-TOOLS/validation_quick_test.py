#!/usr/bin/env python3
"""
ğŸ§ª VALIDACIÃ“N RÃPIDA DE SOLUCIONES
Test inmediato de las correcciones implementadas
"""

import os
import re
import json
import time
from pathlib import Path
from datetime import datetime
from collections import defaultdict

def quick_validation_test():
    """ğŸ§ª Test rÃ¡pido de las soluciones implementadas"""
    print("ğŸ§ª VALIDACIÃ“N RÃPIDA DE SOLUCIONES")
    print("=" * 50)
    
    base_path = Path(".")
    logs_path = base_path / "05-LOGS"
    
    results = {
        'timestamp': datetime.now().isoformat(),
        'tests': {},
        'overall_score': 0,
        'recommendations': []
    }
    
    # TEST 1: Verificar volumen de logs APPLICATION
    print("ğŸ“Š TEST 1: Verificando volumen de logs...")
    app_log = logs_path / "application" / "ict_engine_2025-09-09.log"
    
    if app_log.exists():
        line_count = sum(1 for _ in open(app_log, 'r', encoding='utf-8', errors='ignore'))
        results['tests']['log_volume'] = {
            'current_lines': line_count,
            'target_max': 1000,
            'status': 'PASS' if line_count < 1000 else 'FAIL',
            'severity': 'HIGH' if line_count > 2000 else 'MEDIUM'
        }
        print(f"   ğŸ“ˆ LÃ­neas actuales: {line_count}")
        print(f"   ğŸ¯ Estado: {'âœ… PASS' if line_count < 1000 else 'âŒ FAIL'}")
    else:
        results['tests']['log_volume'] = {'status': 'NOT_FOUND'}
        print("   âš ï¸ Archivo no encontrado")
    
    # TEST 2: Contar errores y crÃ­ticos actuales
    print("\nğŸš¨ TEST 2: Verificando errores crÃ­ticos...")
    error_count = 0
    critical_count = 0
    test_errors = 0
    
    for log_file in logs_path.rglob("*.log"):
        if log_file.stat().st_size > 0:
            try:
                with open(log_file, 'r', encoding='utf-8', errors='ignore') as f:
                    for line in f:
                        if 'ERROR' in line:
                            if 'Prueba de' in line:
                                test_errors += 1
                            else:
                                error_count += 1
                        if 'CRITICAL' in line:
                            critical_count += 1
            except:
                continue
    
    results['tests']['errors'] = {
        'current_errors': error_count,
        'current_criticals': critical_count,
        'test_errors': test_errors,
        'status': 'PASS' if critical_count == 0 and error_count <= 2 else 'FAIL'
    }
    
    print(f"   ğŸ”´ ERRORs: {error_count}")
    print(f"   ğŸš¨ CRITICALs: {critical_count}")
    print(f"   ğŸ§ª Test errors: {test_errors}")
    print(f"   ğŸ¯ Estado: {'âœ… PASS' if critical_count == 0 and error_count <= 2 else 'âŒ FAIL'}")
    
    # TEST 3: Verificar consistencia de timestamps
    print("\nâ° TEST 3: Verificando timestamps...")
    inconsistent_files = 0
    files_checked = 0
    
    for log_file in logs_path.rglob("*.log"):
        if log_file.stat().st_size > 0:
            files_checked += 1
            formats_found = set()
            
            try:
                with open(log_file, 'r', encoding='utf-8', errors='ignore') as f:
                    for i, line in enumerate(f):
                        if i >= 10:  # Solo primeras 10 lÃ­neas
                            break
                        timestamp_match = re.search(r'^\[(.*?)\]', line)
                        if timestamp_match:
                            formats_found.add(len(timestamp_match.group(1)))
                
                if len(formats_found) > 1:
                    inconsistent_files += 1
            except:
                continue
    
    results['tests']['timestamps'] = {
        'files_checked': files_checked,
        'inconsistent_files': inconsistent_files,
        'status': 'PASS' if inconsistent_files == 0 else 'FAIL'
    }
    
    print(f"   ğŸ“ Archivos revisados: {files_checked}")
    print(f"   âš ï¸ Archivos inconsistentes: {inconsistent_files}")
    print(f"   ğŸ¯ Estado: {'âœ… PASS' if inconsistent_files == 0 else 'âŒ FAIL'}")
    
    # TEST 4: Verificar existencia de archivos de configuraciÃ³n
    print("\nâš™ï¸ TEST 4: Verificando configuraciones...")
    config_files = [
        "01-CORE/config/log_throttle_config.json",
        "01-CORE/config/risk_management_config.json", 
        "01-CORE/config/timestamp_config.json",
        "01-CORE/config/log_categorization_rules.json"
    ]
    
    configs_exist = 0
    for config_file in config_files:
        if (base_path / config_file).exists():
            configs_exist += 1
            print(f"   âœ… {config_file}")
        else:
            print(f"   âŒ {config_file}")
    
    results['tests']['configurations'] = {
        'total_configs': len(config_files),
        'configs_exist': configs_exist,
        'status': 'PASS' if configs_exist >= 3 else 'FAIL'
    }
    
    print(f"   ğŸ¯ Estado: {'âœ… PASS' if configs_exist >= 3 else 'âŒ FAIL'}")
    
    # TEST 5: Verificar utilidades creadas
    print("\nğŸ”§ TEST 5: Verificando utilidades...")
    utils_files = [
        "01-CORE/utils/realtime_log_deduplicator.py",
        "01-CORE/utils/log_categorizer.py",
        "01-CORE/risk_management/risk_validator.py",
        "01-CORE/emergency/emergency_handler.py"
    ]
    
    utils_exist = 0
    for util_file in utils_files:
        if (base_path / util_file).exists():
            utils_exist += 1
            print(f"   âœ… {util_file}")
        else:
            print(f"   âŒ {util_file}")
    
    results['tests']['utilities'] = {
        'total_utils': len(utils_files),
        'utils_exist': utils_exist,
        'status': 'PASS' if utils_exist >= 2 else 'FAIL'
    }
    
    print(f"   ğŸ¯ Estado: {'âœ… PASS' if utils_exist >= 2 else 'âŒ FAIL'}")
    
    # CALCULAR SCORE GENERAL
    passed_tests = sum(1 for test in results['tests'].values() 
                      if test.get('status') == 'PASS')
    total_tests = len(results['tests'])
    results['overall_score'] = (passed_tests / total_tests) * 100
    
    # GENERAR RECOMENDACIONES
    if results['tests']['log_volume']['status'] == 'FAIL':
        results['recommendations'].append("ğŸš¨ CRÃTICO: Implementar throttling de logs inmediatamente")
    
    if results['tests']['errors']['status'] == 'FAIL':
        results['recommendations'].append("ğŸš¨ CRÃTICO: Resolver errores y violaciones de riesgo")
    
    if results['tests']['timestamps']['status'] == 'FAIL':
        results['recommendations'].append("ğŸ”¶ ALTO: Estandarizar formatos de timestamp")
    
    if results['tests']['configurations']['status'] == 'FAIL':
        results['recommendations'].append("ğŸ”§ MEDIO: Crear archivos de configuraciÃ³n faltantes")
    
    if results['tests']['utilities']['status'] == 'FAIL':
        results['recommendations'].append("ğŸ”§ MEDIO: Implementar utilidades de sistema faltantes")
    
    # MOSTRAR RESUMEN FINAL
    print("\n" + "=" * 50)
    print("ğŸ“Š RESUMEN DE VALIDACIÃ“N")
    print("=" * 50)
    print(f"âœ… Tests pasados: {passed_tests}/{total_tests}")
    print(f"ğŸ“ˆ Score general: {results['overall_score']:.1f}%")
    
    if results['overall_score'] >= 80:
        print("ğŸ‰ VALIDACIÃ“N EXITOSA - Sistema listo para FASE 1")
        print("âœ… Puede continuar con documentaciÃ³n operacional")
    elif results['overall_score'] >= 60:
        print("âš ï¸ VALIDACIÃ“N PARCIAL - Requiere atenciÃ³n")
        print("ğŸ”§ Implementar correcciones recomendadas")
    else:
        print("ğŸš¨ VALIDACIÃ“N FALLIDA - IntervenciÃ³n requerida")
        print("âš¡ Problemas crÃ­ticos deben resolverse primero")
    
    if results['recommendations']:
        print("\nğŸ¯ RECOMENDACIONES INMEDIATAS:")
        for rec in results['recommendations']:
            print(f"   {rec}")
    
    # PRÃ“XIMOS PASOS ESPECÃFICOS
    print("\nğŸš€ PRÃ“XIMOS PASOS:")
    if results['overall_score'] >= 80:
        print("   1. âœ… Crear quick-start.md")
        print("   2. âœ… Crear troubleshooting.md") 
        print("   3. âœ… Crear emergency-procedures.md")
        print("   4. âœ… Crear production-checklist.md")
    else:
        print("   1. ğŸ”§ Ejecutar log_problems_solver.py")
        print("   2. ğŸ§ª Re-ejecutar este test de validaciÃ³n")
        print("   3. ğŸ“‹ Revisar problemas especÃ­ficos")
        print("   4. âš¡ Repetir hasta score >= 80%")
    
    # Guardar resultados
    try:
        results_file = base_path / "03-DOCUMENTATION" / "reports" / f"validation_quick_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        results_file.parent.mkdir(parents=True, exist_ok=True)
        
        with open(results_file, 'w') as f:
            json.dump(results, f, indent=2)
        
        print(f"\nğŸ’¾ Resultados guardados: {results_file}")
    except Exception as e:
        print(f"âš ï¸ Error guardando resultados: {e}")
    
    return results

if __name__ == "__main__":
    print("ğŸ§ª ICT ENGINE v6.0 - VALIDACIÃ“N RÃPIDA")
    print("=" * 50)
    print("ğŸ¯ Validando estado actual del sistema...")
    print("â±ï¸ Tiempo estimado: 1-2 minutos")
    print()
    
    results = quick_validation_test()
    
    # DecisiÃ³n final
    print("\n" + "=" * 70)
    print("ğŸ¯ DECISIÃ“N FINAL")
    print("=" * 70)
    
    if results['overall_score'] >= 80:
        print("ğŸš€ SISTEMA LISTO PARA FASE 1 DE DOCUMENTACIÃ“N")
        print("âœ¨ Comenzar con quick-start.md inmediatamente")
        print("ğŸ“‹ Problemas crÃ­ticos resueltos satisfactoriamente")
    elif results['overall_score'] >= 60:
        print("âš¡ SISTEMA REQUIERE ATENCIÃ“N ADICIONAL")
        print("ğŸ”§ Ejecutar log_problems_solver.py completo")
        print("ğŸ§ª Re-validar despuÃ©s de correcciones")
    else:
        print("ğŸš¨ SISTEMA REQUIERE INTERVENCIÃ“N CRÃTICA")
        print("âš ï¸ Problemas fundamentales deben resolverse")
        print("ğŸ“ Considerar escalaciÃ³n o reinicio parcial")
    
    print("\nğŸ“š Continuando con el plan de trabajo por fases...")
    print("ğŸ¯ Objetivo: DocumentaciÃ³n operacional crÃ­tica")
