#!/usr/bin/env python3
"""
Aggregate Baseline Reports
==========================

Reads JSON files generated by `scripts/baseline_pattern_scan.py` (pattern: baseline_*_*.json)
under `04-DATA/reports/` and produces a consolidated summary JSON:

- Per symbol + timeframe + detector: count of files, total signals, average/max confidence
- Global summary across all symbols/timeframes
- List of files with errors

Usage (PowerShell):
python -X utf8 .\\scripts\\aggregate_baseline_reports.py -i .\\04-DATA\\reports -o .\\04-DATA\\reports\\baseline_summary.json
"""
from __future__ import annotations
import argparse
import json
from pathlib import Path
from typing import Any, Dict, List, Tuple


def _get_detector_stats(detectors_obj: Dict[str, Any]) -> Dict[str, Dict[str, float]]:
    out: Dict[str, Dict[str, float]] = {}
    for name, d in detectors_obj.items():
        if isinstance(d, dict) and 'error' in d:
            out[name] = {
                'count': 0.0,
                'avg_confidence': 0.0,
                'max_confidence': 0.0,
                'error': 1.0,
            }
            continue
        count = float(d.get('count', 0)) if isinstance(d, dict) else 0.0
        avg_conf = float(d.get('avg_confidence', 0.0)) if isinstance(d, dict) else 0.0
        max_conf = float(d.get('max_confidence', 0.0)) if isinstance(d, dict) else 0.0
        out[name] = {
            'count': count,
            'avg_confidence': avg_conf,
            'max_confidence': max_conf,
        }
    return out


def aggregate_reports(in_dir: Path) -> Dict[str, Any]:
    files = sorted(in_dir.glob('baseline_*_*.json'))
    summary: Dict[str, Any] = {
        'meta': {
            'reports_dir': str(in_dir),
            'files_found': len(files),
        },
        'by_symbol_timeframe': {},
        'global': {
            'files_count': 0,
            'detectors': {},
            'errors': [],
        },
    }

    # Accumulators per (symbol,timeframe,detector)
    acc: Dict[Tuple[str, str, str], Dict[str, float]] = {}

    error_files: List[Dict[str, str]] = []

    for fp in files:
        try:
            with fp.open('r', encoding='utf-8') as f:
                data = json.load(f)
        except Exception as e:
            error_files.append({'file': str(fp), 'error': f'load_error: {e}'})
            continue

        if 'error' in data:
            error_files.append({'file': str(fp), 'error': str(data.get('error'))})
            continue

        meta = data.get('meta', {})
        symbol = str(meta.get('symbol', 'UNKNOWN'))
        timeframe = str(meta.get('timeframe', 'UNKNOWN'))
        detectors = data.get('detectors', {})

        det_stats = _get_detector_stats(detectors)
        for det_name, stats in det_stats.items():
            key = (symbol, timeframe, det_name)
            entry = acc.setdefault(key, {
                'files_count': 0.0,
                'total_signals': 0.0,
                'sum_avg_conf': 0.0,
                'max_confidence': 0.0,
                'error_count': 0.0,
            })
            entry['files_count'] += 1.0
            entry['total_signals'] += stats.get('count', 0.0)
            entry['sum_avg_conf'] += stats.get('avg_confidence', 0.0)
            entry['max_confidence'] = max(entry['max_confidence'], stats.get('max_confidence', 0.0))
            if stats.get('error', 0.0) == 1.0:
                entry['error_count'] += 1.0

    # Build by_symbol_timeframe structure
    for (symbol, timeframe, det), v in acc.items():
        avg_conf_over_files = (v['sum_avg_conf'] / v['files_count']) if v['files_count'] > 0 else 0.0
        summary.setdefault('by_symbol_timeframe', {}).setdefault(symbol, {}).setdefault(timeframe, {})[det] = {
            'files_count': int(v['files_count']),
            'total_signals': int(v['total_signals']),
            'avg_confidence': round(avg_conf_over_files, 2),
            'max_confidence': round(v['max_confidence'], 2),
            'error_files': int(v['error_count']),
        }

    # Global summary
    summary['global']['files_count'] = len(files)
    # Aggregate global per detector
    global_acc: Dict[str, Dict[str, float]] = {}
    for (_, _, det), v in acc.items():
        e = global_acc.setdefault(det, {
            'files_count': 0.0,
            'total_signals': 0.0,
            'sum_avg_conf': 0.0,
            'max_confidence': 0.0,
            'error_files': 0.0,
        })
        e['files_count'] += v['files_count']
        e['total_signals'] += v['total_signals']
        e['sum_avg_conf'] += v['sum_avg_conf']
        e['max_confidence'] = max(e['max_confidence'], v['max_confidence'])
        e['error_files'] += v['error_count']

    for det, v in global_acc.items():
        avg_conf_over_files = (v['sum_avg_conf'] / v['files_count']) if v['files_count'] > 0 else 0.0
        summary['global'].setdefault('detectors', {})[det] = {
            'files_count': int(v['files_count']),
            'total_signals': int(v['total_signals']),
            'avg_confidence': round(avg_conf_over_files, 2),
            'max_confidence': round(v['max_confidence'], 2),
            'error_files': int(v['error_files']),
        }

    summary['global']['errors'] = error_files
    return summary


def main() -> None:
    parser = argparse.ArgumentParser(description='Aggregate baseline_* reports and produce a consolidated summary JSON.')
    parser.add_argument('-i', '--in-dir', default='04-DATA/reports', help='Input reports directory')
    parser.add_argument('-o', '--out-file', default='04-DATA/reports/baseline_summary.json', help='Output summary JSON file')
    args = parser.parse_args()

    in_dir = Path(args.in_dir)
    out_file = Path(args.out_file)
    out_file.parent.mkdir(parents=True, exist_ok=True)

    summary = aggregate_reports(in_dir)
    with out_file.open('w', encoding='utf-8') as f:
        json.dump(summary, f, indent=2)

    print(json.dumps({'output_file': str(out_file), 'files_found': summary['meta']['files_found']}, indent=2))


if __name__ == '__main__':
    main()
